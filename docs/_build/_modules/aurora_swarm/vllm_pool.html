<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>aurora_swarm.vllm_pool &#8212; Aurora Swarm</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=27fed22d" />
    <script src="../../_static/documentation_options.js?v=01f34227"></script>
    <script src="../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for aurora_swarm.vllm_pool</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;VLLMPool — AgentPool subclass for vLLM OpenAI-compatible endpoints.</span>

<span class="sd">vLLM exposes an OpenAI-compatible chat completions API at</span>
<span class="sd">``/v1/chat/completions``.  This pool overrides :meth:`post` to speak</span>
<span class="sd">that protocol instead of the simpler ``/generate`` endpoint used by</span>
<span class="sd">the base :class:`AgentPool`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">aiohttp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncOpenAI</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">aurora_swarm.hostfile</span><span class="w"> </span><span class="kn">import</span> <span class="n">AgentEndpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aurora_swarm.pool</span><span class="w"> </span><span class="kn">import</span> <span class="n">AgentPool</span><span class="p">,</span> <span class="n">Response</span>


<div class="viewcode-block" id="VLLMPool">
<a class="viewcode-back" href="../../api.html#aurora_swarm.VLLMPool">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMPool</span><span class="p">(</span><span class="n">AgentPool</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Agent pool that communicates via vLLM&#39;s OpenAI-compatible API.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    endpoints:</span>
<span class="sd">        Agent endpoints (host + port where vLLM is listening).</span>
<span class="sd">    model:</span>
<span class="sd">        Model identifier passed in the ``&quot;model&quot;`` field of every</span>
<span class="sd">        request (e.g. ``&quot;openai/gpt-oss-120b&quot;``).</span>
<span class="sd">    max_tokens:</span>
<span class="sd">        Maximum tokens to generate per request (default context).</span>
<span class="sd">        Can be overridden via ``AURORA_SWARM_MAX_TOKENS`` env var.</span>
<span class="sd">    max_tokens_aggregation:</span>
<span class="sd">        Maximum tokens for aggregation/reduce steps (larger prompts).</span>
<span class="sd">        Can be overridden via ``AURORA_SWARM_MAX_TOKENS_AGGREGATION`` env var.</span>
<span class="sd">        Defaults to 2 * max_tokens if not specified.</span>
<span class="sd">    model_max_context:</span>
<span class="sd">        Model&#39;s maximum context length. If None, will be fetched from</span>
<span class="sd">        vLLM&#39;s ``/v1/models`` endpoint on first request. Can be overridden</span>
<span class="sd">        via ``AURORA_SWARM_MODEL_MAX_CONTEXT`` env var.</span>
<span class="sd">    buffer:</span>
<span class="sd">        Safety margin (in tokens) for dynamic sizing to account for</span>
<span class="sd">        reasoning overhead. Defaults to 512.</span>
<span class="sd">    use_batch:</span>
<span class="sd">        If True, use batch prompting via the completions API for</span>
<span class="sd">        send_all_batched. If False, fall back to individual requests.</span>
<span class="sd">        Defaults to True.</span>
<span class="sd">    concurrency:</span>
<span class="sd">        Maximum number of in-flight requests.</span>
<span class="sd">    connector_limit:</span>
<span class="sd">        Maximum TCP connections in the aiohttp pool.</span>
<span class="sd">    timeout:</span>
<span class="sd">        Per-request timeout in seconds.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">endpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AgentEndpoint</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;openai/gpt-oss-120b&quot;</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_tokens_aggregation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_max_context</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">buffer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">use_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">concurrency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
        <span class="n">connector_limit</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">300.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">endpoints</span><span class="p">,</span>
            <span class="n">concurrency</span><span class="o">=</span><span class="n">concurrency</span><span class="p">,</span>
            <span class="n">connector_limit</span><span class="o">=</span><span class="n">connector_limit</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch</span> <span class="o">=</span> <span class="n">use_batch</span>
        
        <span class="c1"># Load from environment with fallbacks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">max_tokens</span>
            <span class="ow">or</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;AURORA_SWARM_MAX_TOKENS&quot;</span><span class="p">,</span> <span class="s2">&quot;512&quot;</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens_aggregation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">max_tokens_aggregation</span>
            <span class="ow">or</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;AURORA_SWARM_MAX_TOKENS_AGGREGATION&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">model_max_context</span>
            <span class="ow">or</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AURORA_SWARM_MODEL_MAX_CONTEXT&quot;</span><span class="p">])</span> <span class="k">if</span> <span class="s2">&quot;AURORA_SWARM_MODEL_MAX_CONTEXT&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="n">buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="c1"># Create OpenAI clients for each endpoint (for batch requests)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_openai_clients</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">AsyncOpenAI</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoints</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_openai_clients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">AsyncOpenAI</span><span class="p">(</span>
                <span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ep</span><span class="o">.</span><span class="n">url</span><span class="si">}</span><span class="s2">/v1&quot;</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;EMPTY&quot;</span><span class="p">,</span>  <span class="c1"># vLLM convention</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="c1"># -- model metadata -------------------------------------------------------</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_get_model_max_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fetch the model&#39;s max context length from vLLM /v1/models endpoint.</span>
<span class="sd">        </span>
<span class="sd">        Cached after first call. Returns a sensible default if fetch fails.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Return cached value if available</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span>
        
        <span class="c1"># Return explicitly configured value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context</span>
        
        <span class="c1"># Fetch from vLLM API</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">session</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_session</span><span class="p">()</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ep</span><span class="o">.</span><span class="n">url</span><span class="si">}</span><span class="s2">/v1/models&quot;</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientTimeout</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="mf">10.0</span><span class="p">),</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
                <span class="c1"># Find our model in the list</span>
                <span class="k">for</span> <span class="n">model_info</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="p">[]):</span>
                    <span class="k">if</span> <span class="n">model_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">:</span>
                        <span class="n">max_len</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_model_len&quot;</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">max_len</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span> <span class="o">=</span> <span class="n">max_len</span>
                            <span class="k">return</span> <span class="n">max_len</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>  <span class="c1"># Fall back to default</span>
        
        <span class="c1"># Default fallback (131072 is common for many models)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span> <span class="o">=</span> <span class="mi">131072</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span>

    <span class="c1"># -- core request (OpenAI chat completions) ------------------------------</span>

<div class="viewcode-block" id="VLLMPool.post">
<a class="viewcode-back" href="../../api.html#aurora_swarm.VLLMPool.post">[docs]</a>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">post</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Response</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Send *prompt* via the OpenAI chat-completions API on the agent.</span>

<span class="sd">        The prompt is wrapped as a single ``user`` message.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        agent_index:</span>
<span class="sd">            Index of the agent to send the prompt to.</span>
<span class="sd">        prompt:</span>
<span class="sd">            The prompt text.</span>
<span class="sd">        max_tokens:</span>
<span class="sd">            Optional override for max tokens. If None, uses dynamic sizing</span>
<span class="sd">            based on prompt length and model context limit.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoints</span><span class="p">[</span><span class="n">agent_index</span><span class="p">]</span>
        <span class="n">session</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_session</span><span class="p">()</span>
        
        <span class="c1"># Compute max_tokens dynamically if not explicitly provided</span>
        <span class="k">if</span> <span class="n">max_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Get model&#39;s max context length</span>
            <span class="n">model_max</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_model_max_context</span><span class="p">()</span>
            
            <span class="c1"># Estimate prompt tokens (rough heuristic: 1 token ≈ 4 chars)</span>
            <span class="n">prompt_est</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span>
            
            <span class="c1"># Dynamic sizing: never exceed model capacity</span>
            <span class="c1"># Use default max_tokens as the preferred cap</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens</span><span class="p">,</span>
                <span class="nb">max</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">model_max</span> <span class="o">-</span> <span class="n">prompt_est</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">max_tokens</span>
        
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_semaphore</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ep</span><span class="o">.</span><span class="n">url</span><span class="si">}</span><span class="s2">/v1/chat/completions&quot;</span><span class="p">,</span>
                    <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span>
                        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
                        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">tokens</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientTimeout</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span><span class="p">),</span>
                <span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
                    
                    <span class="c1"># Check for error response</span>
                    <span class="k">if</span> <span class="n">resp</span><span class="o">.</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
                        <span class="n">error_msg</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;error&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;HTTP </span><span class="si">{</span><span class="n">resp</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="k">return</span> <span class="n">Response</span><span class="p">(</span>
                            <span class="n">success</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                            <span class="n">error</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;API error: </span><span class="si">{</span><span class="n">error_msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                            <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                        <span class="p">)</span>
                    
                    <span class="c1"># Check for expected response structure</span>
                    <span class="k">if</span> <span class="s2">&quot;choices&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">data</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">]:</span>
                        <span class="k">return</span> <span class="n">Response</span><span class="p">(</span>
                            <span class="n">success</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                            <span class="n">error</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Invalid response structure: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                            <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                        <span class="p">)</span>
                    
                    <span class="n">message</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">]</span>
                    <span class="n">text</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reasoning_content&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">return</span> <span class="n">Response</span><span class="p">(</span>
                        <span class="n">success</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                        <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Response</span><span class="p">(</span>
                    <span class="n">success</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                <span class="p">)</span></div>


<div class="viewcode-block" id="VLLMPool.post_batch">
<a class="viewcode-back" href="../../api.html#aurora_swarm.VLLMPool.post_batch">[docs]</a>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">post_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">agent_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Response</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Send multiple prompts to one agent via the completions API.</span>

<span class="sd">        Uses the OpenAI completions endpoint which supports batch prompts</span>
<span class="sd">        (a list of strings). This reduces N HTTP requests to 1.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        agent_index:</span>
<span class="sd">            Index of the agent to send prompts to.</span>
<span class="sd">        prompts:</span>
<span class="sd">            List of prompts to send in one batch.</span>
<span class="sd">        max_tokens:</span>
<span class="sd">            Optional override for max tokens. If None, uses dynamic sizing</span>
<span class="sd">            based on average prompt length.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list[Response]</span>
<span class="sd">            One Response per prompt, in the same order as the input.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">prompts</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="n">ep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoints</span><span class="p">[</span><span class="n">agent_index</span><span class="p">]</span>
        <span class="n">client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_openai_clients</span><span class="p">[</span><span class="n">agent_index</span><span class="p">]</span>

        <span class="c1"># Compute max_tokens dynamically if not explicitly provided</span>
        <span class="k">if</span> <span class="n">max_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Get model&#39;s max context length</span>
            <span class="n">model_max</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_model_max_context</span><span class="p">()</span>

            <span class="c1"># Estimate tokens based on average prompt length</span>
            <span class="n">avg_prompt_len</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">)</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
            <span class="n">prompt_est</span> <span class="o">=</span> <span class="n">avg_prompt_len</span> <span class="o">//</span> <span class="mi">4</span>

            <span class="c1"># Dynamic sizing: never exceed model capacity</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens</span><span class="p">,</span>
                <span class="nb">max</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">model_max</span> <span class="o">-</span> <span class="n">prompt_est</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">max_tokens</span>

        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_semaphore</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Call completions API with batch prompts</span>
                <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span>
                    <span class="n">prompt</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
                    <span class="n">max_tokens</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># Map choices to Response objects</span>
                <span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Response</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">choice</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">):</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">Response</span><span class="p">(</span>
                            <span class="n">success</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">text</span><span class="o">=</span><span class="n">choice</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
                            <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">results</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                <span class="c1"># On error, return failed Response for each prompt</span>
                <span class="k">return</span> <span class="p">[</span>
                    <span class="n">Response</span><span class="p">(</span>
                        <span class="n">success</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                        <span class="n">error</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">agent_index</span><span class="o">=</span><span class="n">agent_index</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">prompts</span>
                <span class="p">]</span></div>


<div class="viewcode-block" id="VLLMPool.send_all_batched">
<a class="viewcode-back" href="../../api.html#aurora_swarm.VLLMPool.send_all_batched">[docs]</a>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">send_all_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Response</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Send prompts using batch API, grouping by target agent.</span>

<span class="sd">        Groups prompts by their target agent (round-robin based on index),</span>
<span class="sd">        then sends one batched request per agent. Reconstructs results in</span>
<span class="sd">        input order.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        prompts:</span>
<span class="sd">            List of prompts to send.</span>
<span class="sd">        max_tokens:</span>
<span class="sd">            Optional max tokens override.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list[Response]</span>
<span class="sd">            Responses in the same order as input prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">prompts</span><span class="p">:</span>
            <span class="c1"># Fall back to non-batched send_all</span>
            <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">send_all</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>

        <span class="c1"># Group prompts by target agent</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">agent_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>
            <span class="k">if</span> <span class="n">agent_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
                <span class="n">groups</span><span class="p">[</span><span class="n">agent_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">groups</span><span class="p">[</span><span class="n">agent_idx</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">prompt</span><span class="p">))</span>

        <span class="c1"># Send batched requests concurrently</span>
        <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">send_group</span><span class="p">(</span><span class="n">agent_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Response</span><span class="p">]]:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Send batch to one agent, return (original_index, response) pairs.&quot;&quot;&quot;</span>
            <span class="n">group_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">items</span><span class="p">]</span>
            <span class="n">responses</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_batch</span><span class="p">(</span><span class="n">agent_idx</span><span class="p">,</span> <span class="n">group_prompts</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[(</span><span class="n">items</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">responses</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">))]</span>

        <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">send_group</span><span class="p">(</span><span class="n">agent_idx</span><span class="p">,</span> <span class="n">items</span><span class="p">)</span> <span class="k">for</span> <span class="n">agent_idx</span><span class="p">,</span> <span class="n">items</span> <span class="ow">in</span> <span class="n">groups</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">all_results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

        <span class="c1"># Flatten and sort by original index</span>
        <span class="n">indexed_responses</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Response</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">result_group</span> <span class="ow">in</span> <span class="n">all_results</span><span class="p">:</span>
            <span class="n">indexed_responses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">result_group</span><span class="p">)</span>
        <span class="n">indexed_responses</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Extract responses in order</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">resp</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">resp</span> <span class="ow">in</span> <span class="n">indexed_responses</span><span class="p">]</span></div>


    <span class="c1"># -- sub-pool override ---------------------------------------------------</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sub_pool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">endpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AgentEndpoint</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;VLLMPool&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a child VLLMPool sharing concurrency settings.&quot;&quot;&quot;</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">VLLMPool</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">VLLMPool</span><span class="p">)</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_endpoints</span> <span class="o">=</span> <span class="n">endpoints</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_concurrency</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concurrency</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_connector_limit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connector_limit</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_semaphore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_semaphore</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_session</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_use_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_max_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_max_tokens_aggregation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_tokens_aggregation</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_model_max_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_model_max_context_cached</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_max_context_cached</span>
        <span class="n">child</span><span class="o">.</span><span class="n">_openai_clients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_openai_clients</span>  <span class="c1"># Share clients</span>
        <span class="k">return</span> <span class="n">child</span></div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Aurora Swarm</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../batch_prompting.html">Batch Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../context_length.html">Context length configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;Aurora Swarm contributors.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.1.0</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>